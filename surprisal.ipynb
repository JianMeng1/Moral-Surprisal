{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc49731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.75.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.9.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\r\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\r\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "#!pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "!pip install openai\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b09cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = None\n",
    "openai.api_key = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec7e9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Calculates surprisal and other \\\n",
    "                                    metrics (in development) of transformers language models')\n",
    "\n",
    "    parser.add_argument('--Stimuli.txt', '-i', type=str,\n",
    "                        help='Stimuli to test.')\n",
    "    parser.add_argument('--stimuli_list', '-ii', type=str,\n",
    "                        help='Path to file containing list of stimulus files to test.')\n",
    "    parser.add_argument('--output_directory','-o', type=str, required = True,\n",
    "                        help='Output directory.')\n",
    "    parser.add_argument('--gpt-3.5-turbo','-m', type=str,\n",
    "                        help='The name of the GPT-3 model to run.')\n",
    "    parser.add_argument('--model_list','-mm', type=str,\n",
    "                        help='Path to file with a list of GPT-3 models to run.')\n",
    "    parser.add_argument('--key','-k', type=str,\n",
    "                        help='Your OpenAI API key.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9536fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    stimuli=\"--Stimuli.txt\",\n",
    "    stimuli_list=None,\n",
    "    output_directory=\"/Users/mengjian/Desktop/Comp Ling/Final project\",\n",
    "    model = \"text-davinci-003\",\n",
    "    model_list=None,\n",
    "    key= \"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95a5c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mkey\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "openai.api_key = args.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51456f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_args(args):\n",
    "    \n",
    "    try:\n",
    "        output_directory = args.output_directory\n",
    "    except:\n",
    "        print(\"Error: Please specify a valid output directory.\")\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        try:\n",
    "            os.makedirs(output_directory)\n",
    "        except:\n",
    "            print(\"Error: Cannot create output directory (Note: output directory does not already exist).\")\n",
    "    \n",
    "    if args.model_list:\n",
    "        try:\n",
    "            assert os.path.exists(args.model_list)\n",
    "            with open(args.model_list, \"r\") as f:\n",
    "                model_list = f.read().splitlines()\n",
    "        except:\n",
    "            print(\"Error: 'model_list' argument does not have a valid path. Trying to use individual specified model.\")\n",
    "            try:\n",
    "                assert args.model\n",
    "                model_list = [args.model]\n",
    "            except:\n",
    "                print(\"Error: No model specified\")\n",
    "    else:\n",
    "        try:\n",
    "            assert args.model\n",
    "            model_list = [args.model]\n",
    "        except:\n",
    "            print(\"Error: No model specified\") \n",
    "            \n",
    "            \n",
    "    if args.stimuli_list:\n",
    "        try:\n",
    "            assert os.path.exists(args.stimuli_list)\n",
    "            with open(args.stimuli_list, \"r\") as f:\n",
    "                stimulus_file_list = f.read().splitlines()\n",
    "        except:\n",
    "            print(\"Error: 'stimuli_list' argument does not have a valid path. Trying to use individual stimulus set.\")\n",
    "            try:\n",
    "                assert args.stimuli\n",
    "                stimulus_file_list = [args.stimuli]\n",
    "            except:\n",
    "                print(\"Error: No stimuli specified\")\n",
    "    else:\n",
    "        try:\n",
    "            assert args.stimuli\n",
    "            stimulus_file_list = [args.stimuli]\n",
    "        except:\n",
    "            print(\"Error: No stimuli specified\")  \n",
    "\n",
    "    try:\n",
    "        if openai.api_key==None:\n",
    "            assert args.key\n",
    "            openai.api_key = args.key\n",
    "\n",
    "    except:\n",
    "        print(\"No API Key. Unable to run GPT-3.\")   \n",
    "\n",
    "                \n",
    "    return(output_directory,model_list,stimulus_file_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366653cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(output_directory,model_list,stimulus_file_list):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    for j in range(len(model_list)):\n",
    "        model_name = model_list[j]\n",
    "        model_name_cleaned = \"gpt3\"+ model_name.replace(\"-\",\"_\")\n",
    "        for i in range(len(stimulus_file_list)):\n",
    "            stimuli_name = stimulus_file_list[i].split('/')[-1].split('.')[0] \n",
    "            filename = output_directory + \"/\" + stimuli_name + \".\" + \"surprisal\" + \".\" + model_name_cleaned + \".causal.output\"\n",
    "            with open(filename,\"w\") as f:\n",
    "                f.write(\"FullSentence\\tSentence\\tTargetWords\\tSurprisal\\tNumTokens\\n\")\n",
    "            \n",
    "            with open(stimulus_file_list[i],'r') as f:\n",
    "                stimulus_list = f.read().splitlines() \n",
    "            for j in range(len(stimulus_list)):\n",
    "                try:\n",
    "                    stimulus = stimulus_list[j]\n",
    "                    stimulus_spaces = stimulus.replace(\"*\", \"α\")\n",
    "                    stimulus_spaces = stimulus_spaces.replace(\" α\", \"α \")\n",
    "                    encoded_stimulus = tokenizer.encode(stimulus_spaces)\n",
    "\n",
    "                    if (len(tokenizer.tokenize(\"aα\"))==2): \n",
    "                        dummy_var_idxs = np.where((np.array(encoded_stimulus)==tokenizer.encode(\"α\")[-1]) | (np.array(encoded_stimulus)==tokenizer.encode(\"aα\")[-1]))[0]\n",
    "                        preceding_context = encoded_stimulus[:dummy_var_idxs[0]]\n",
    "                        target_words = encoded_stimulus[dummy_var_idxs[0]+1:dummy_var_idxs[1]]\n",
    "                        following_words = encoded_stimulus[dummy_var_idxs[1]+1:]   \n",
    "                        \n",
    "                    stimulus_cleaned = stimulus.replace(\"*\",\"\")\n",
    "                    \n",
    "                    output = openai.Completion.create(\n",
    "                            engine = model_name,\n",
    "                            prompt = stimulus_cleaned,\n",
    "                            max_tokens = 0,\n",
    "                            temperature = 0,\n",
    "                            top_p = 1,\n",
    "                            n = 1,\n",
    "                            stream = False,\n",
    "                            logprobs = 1,\n",
    "                            stop = \"\\n\",\n",
    "                            echo = True\n",
    "                            )\n",
    "                    logprob = output.to_dict()['choices'][0].to_dict()['logprobs']\n",
    "                    \n",
    "                    surprisal_list = logprob[\"token_logprobs\"][len(preceding_context):len(preceding_context)+len(target_words)]\n",
    "                    \n",
    "                    if surprisal_list[0]==None:\n",
    "                        print(\"Problem with stimulus on line {0}: {1}\\nCannot process the first token in a sentence/sequence.\\n\".format(str(j+1),stimulus_list[j]))\n",
    "                    else:\n",
    "                        sentence = tokenizer.decode(preceding_context+target_words)\n",
    "                        target_string = \"\".join(logprob[\"tokens\"][len(preceding_context):len(preceding_context)+len(target_words)])\n",
    "                        surprisal = -np.log2(np.exp(np.sum(surprisal_list)))\n",
    "                        num_tokens = len(target_words)\n",
    "                        with open(filename,\"a\") as f:\n",
    "                            f.write(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\n\".format(\n",
    "                            stimulus.replace(\"*\",\"\"),\n",
    "                            sentence,\n",
    "                            target_string,\n",
    "                            surprisal,\n",
    "                            num_tokens\n",
    "                            ))\n",
    "                except:\n",
    "                    print(\"Problem with stimulus on line {0}: {1}\\n\".format(str(j+1),stimulus_list[j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c50ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #args = parse_args()\n",
    "\n",
    "\n",
    "    try:\n",
    "        output_directory,model_list,stimulus_file_list = process_args(args)\n",
    "    except:\n",
    "        \"Error: Make sure you include arguments for the stimuli, output directory, GPT-3 models, and API key.\"\n",
    "        return False\n",
    "\n",
    "\n",
    "    try:\n",
    "        run_models(output_directory,model_list,stimulus_file_list)\n",
    "    except:\n",
    "        print(\"Error: issue with stimuli, output directory, GPT-3 models chosen, or API key.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fed143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new)",
   "language": "python",
   "name": "python_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
